ig_records = []  # each entry: {'ig': float, 'questions_remaining': int}
# Logging setup
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# -------------------------------
# New Prompt Definitions and Setup
# -------------------------------

# Existing prompts for Guesser and Oracle (if desired, you can keep the existing ones for other flows)
ynpg = "You are named Guesser. You are trying to guess what physical object Oracle is thinking of by asking clarifying questions and making explicit guesses."
ynpo = "You are named Oracle. Guesser will ask questions and make guesses about the physical object you are thinking of. Answer in complete, natural language sentences."

# Guesser prompt.
opg = (
    "You are named Guesser. Your objective is to narrow down which physical object Oracle is thinking of by both asking clarifying yes/no questions and making explicit guesses when you are confident. "
    "At each turn, you will receive an aggregated list of candidate attributes generated by an external Candidate Generator. These attributes represent common features across possible objects derived from the conversation clues. "
    "For example, if the candidate attributes include 'metal, sharp, portable', you might ask: 'Does it have a metallic body used for cutting?' or explicitly guess: 'Is it a knife?'. "
    "Use these candidate attributes to inform your next move. Your response should be in natural language and must begin with 'Guesser said:' followed by your clarifying question or explicit guess."
)

# Oracle prompt.
opo = (
    "You are named Oracle. Your role is to answer the questions and guesses posed by Guesser in natural, complete sentences. "
    "When Guesser asks a yes/no question, respond with a natural language answer such as 'No, it is not a key.' or 'Yes, it is used for ...'. "
    "If Guesser makes an explicit guess that is correct, reply with 'Correct!' (optionally with a brief comment), otherwise provide a natural explanation of why the guess is incorrect. "
    "Do not ask any questions or offer unsolicited information. Your response must begin with 'Oracle said:' followed by your answer."
)

# -------------------------------
# Existing Helper Functions
# -------------------------------

def clean_candidate(candidate: str) -> str:
    """
    Cleans the candidate string to extract a valid physical object name by removing extraneous leading words and punctuation.
    This function:
      1. Strips whitespace and surrounding quotes/punctuation.
      2. If a colon is present, keeps only the text after the colon.
      3. Removes extraneous leading words (e.g., 'okay', 'ok', 'since', 'it's likely', 'um', 'uh', 'hmm', 'well').
      4. Removes leading articles such as 'a' or 'an'.
      5. Performs final cleanup of any residual punctuation.
    """
    import re
    candidate = candidate.strip().strip('\"\'“”‘’')
    if ':' in candidate:
        candidate = candidate.split(':', 1)[1].strip()
    # Remove extraneous leading words and punctuation.
    pattern = r'^(okay|ok|since|it\'?s likely|um|uh|hmm|well)[\s,\-.:;]+'
    candidate = re.sub(pattern, '', candidate, flags=re.IGNORECASE)
    # Remove leading articles.
    candidate = re.sub(r'^(a|an)\s+', '', candidate, flags=re.IGNORECASE)
    candidate = candidate.strip().strip('.,;:!?\'\"')
    candidate = candidate.replace(" ", "_")
    return candidate

import random
from query_gpt import ChatBot
from query_gpt import MultiChatBot
from huggingface_hub import login
import transformers
import torch
ALLOWED_RELATIONS = ['UsedFor', 'HasProperty', 'AtLocation', 'IsA']
TOTAL_RELATIONS = [
  "Antonym",
  "AtLocation",
  "CapableOf",
  "Causes",
  "CausesDesire",
  "CreatedBy",
  "DefinedAs",
  "DerivedFrom",
  "Desires",
  "DistinctFrom",
  "Entails",
  "FormOf",
  "HasA",
  "HasContext",
  "HasFirstSubevent",
  "HasLastSubevent",
  "HasPrerequisite",
  "HasProperty",
  "HasSubevent",
  "InstanceOf",
  "IsA",
  "LocatedNear",
  "MadeOf",
  "MannerOf",
  "MotivatedByGoal",
  "NotCapableOf",
  "NotDesires",
  "NotHasProperty",
  "PartOf",
  "ReceivesAction",
  "RelatedTo",
  "SimilarTo",
  "SymbolOf",
  "Synonym",
  "UsedFor",
  "dbpedia/capital",
  "dbpedia/field",
  "dbpedia/genre",
  "dbpedia/genus",
  "dbpedia/influencedBy",
  "dbpedia/knownFor",
  "dbpedia/language",
  "dbpedia/leader",
  "dbpedia/occupation",
  "dbpedia/product"
]

PROVIDER = 'gemini'
MODEL = 'gemini-2.0-flash-lite'


# -------------------------------
# Shared requests session with retry/backoff for ConceptNet API
# -------------------------------
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Configure a shared session with retry/backoff
session = requests.Session()
retry = Retry(
    total=3,
    backoff_factor=1,
    status_forcelist=[429, 500, 502, 503, 504],
    allowed_methods=["GET"]
)
session.mount("https://", HTTPAdapter(max_retries=retry))

# -------------------------------
# ConceptNet semantic mapping setup
# -------------------------------
import pandas as pd

try:
    # Only load columns we need for mapping
    edges_df = pd.read_csv('conceptnet_english_edges.csv', usecols=['relation','end_node'])
except Exception as e:
    logger.error("Failed to load ConceptNet edges CSV: %s", e)
    edges_df = None

# --- Semantic similarity model setup for mapping responses to ConceptNet URIs ---
from sentence_transformers import SentenceTransformer
import numpy as np
import os
import pickle

# Preload embedding model
embed_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')

# Cache embeddings so we compute only once
EMBED_CACHE = 'conceptnet_label_embeddings.pkl'
if edges_df is not None:
    if os.path.exists(EMBED_CACHE):
        # Load from disk
        with open(EMBED_CACHE, 'rb') as f:
            choices, labels, label_embeddings = pickle.load(f)
        # Derive uris from choices if needed
        uris = [uri for uri in choices['end_node'].tolist()]
    else:
        # Compute and cache
        choices = edges_df[['relation','end_node']].dropna().drop_duplicates()
        uris = choices['end_node'].tolist()
        labels = [uri.split('/')[3].replace('_',' ') for uri in uris]
        label_embeddings = embed_model.encode(labels, convert_to_tensor=True)
        with open(EMBED_CACHE, 'wb') as f:
            pickle.dump((choices, labels, label_embeddings), f)
else:
    choices = None
    uris = []
    labels = []
    label_embeddings = None

# -----------------------------------------
# Semantic mapping: response to (relation, URI) using embeddings
# -----------------------------------------
def map_response_to_uris(response: str, threshold: float = 0.6):
    """
    Map Oracle response to one or more (relation, URI) pairs via embedding similarity.
    Returns all matches with similarity >= threshold; if none, returns the single best match.
    """
    if edges_df is None or label_embeddings is None:
        concept = response.split()[-1].lower()
        return [(None, f"/c/en/{concept}")]
    import torch
    query_emb = embed_model.encode(response, convert_to_tensor=True)
    sims = torch.nn.functional.cosine_similarity(query_emb, label_embeddings)
    sims_list = sims.cpu().tolist()
    # find indices above threshold
    matched = [i for i, s in enumerate(sims_list) if s >= threshold]
    if not matched:
        best_idx = max(range(len(sims_list)), key=lambda i: sims_list[i])
        matched = [best_idx]
    results = []
    for idx in matched:
        row = choices.iloc[idx]
        results.append((row['relation'], row['end_node']))
    return results

def getObs(obs, res, n):
    objects = open(obs, 'r', encoding='utf8').read().split("\n")
    open(res, "w", encoding='utf8').write("\n".join(random.sample(objects, n)))

def objAtrGPT(obs, res, res2):
    objects = open(obs, 'r', encoding='utf8').read().split("\n")
    attributes = open('attributes.txt', 'r', encoding='utf8').read().replace("\n", ",")
    gpt = ChatBot("You are an expert annotator.", model=MODEL, provider=PROVIDER)
    for i in range(len(objects)):
        response = gpt("Describe a " + objects[i] + " using only the following physical attributes: " + attributes + ". Do not name the object in the description.").replace("\n", "")
        open(res, "a", encoding='utf8').write(response + "\n")
        response = gpt("Describe a " + objects[i] + " using only its physical attributes. Do not name the object in the description.").replace("\n", "")
        open(res2, "a", encoding='utf8').write(response + "\n")

import functools
@functools.lru_cache(maxsize=None)
def queryConceptNet(candidate, allowed_rels_key=None):
    # allowed_rels_key is a comma-separated string key for caching
    allowed_rels = set(allowed_rels_key.split(',')) if allowed_rels_key else TOTAL_RELATIONS
    base_url = "https://api.conceptnet.io/c/en/"
    candidate_url = base_url + candidate.lower()
    try:
        resp = session.get(candidate_url, timeout=5)
        data = resp.json()
    except Exception as e:
        logger.error("Failed ConceptNet query for %s: %s", candidate, e)
        return ""
    attributes = []
    for edge in data.get("edges", []):
        rel = edge.get("rel", {}).get("label", "")
        if rel in allowed_rels:
            end_label = edge.get("end", {}).get("label", "")
            if end_label and end_label.lower() != candidate.lower():
                attributes.append(end_label.lower())
    unique_attrs = list(set(attributes))
    return ", ".join(unique_attrs)

from collections import Counter
import math

def aggregateAttributes(candidates, allowed_rels={"MadeOf", "UsedFor", "HasProperty"}, k=None, use_entropy=False):
    # Build a count of how many candidates contain each attribute
    counter = Counter()
    for candidate in candidates:
        cleaned_candidate = clean_candidate(candidate)
        if not cleaned_candidate:
            continue
        attrs = queryConceptNet(cleaned_candidate)
        # unique per candidate
        attrs_set = {x.strip() for x in attrs.split(",") if x.strip()}
        for attr in attrs_set:
            counter[attr] += 1

    num_candidates = len(candidates)
    if num_candidates == 0:
        if use_entropy:
            return "", 0.0
        return ""

    if use_entropy:
        best_attr = None
        best_ig = -1.0
        # Compute information gain for each attr based on split among candidates
        for attr, m in counter.items():
            # candidates with attr = m, without = num_candidates - m
            if m == 0 or m == num_candidates:
                ig = 0.0
            else:
                # IG = H(C) - expected H(C|attr)
                # H(C) = log2(N)
                Hc = math.log2(num_candidates)
                # H(C|present)=log2(m), H(C|absent)=log2(N-m)
                Hp = math.log2(m)
                Ha = math.log2(num_candidates - m)
                ig = Hc - (m/num_candidates)*Hp - ((num_candidates-m)/num_candidates)*Ha
            if ig > best_ig:
                best_ig = ig
                best_attr = attr
        return best_attr or "", best_ig

    # fallback to frequency-based top-k
    if k:
        attrs_list = [attr for attr, _ in counter.most_common(k)]
    return ", ".join(attrs_list)

# -------------------------------
# Alternative: Use ConceptNet relation sampling
# -------------------------------
import random

from concurrent.futures import ThreadPoolExecutor, as_completed

def generateCandidatesByRelations(positive_pairs, negative_pairs, n=None, threshold=10):
    """
    Pulls concepts from ConceptNet that satisfy positive (relation, concept) pairs (scoring by how many pairs each candidate matches)
    and excludes those matching any negative pairs. Returns top-n candidates by score.
    Parallelized for faster API queries.
    """
    from collections import Counter
    score_counter = Counter()

    # Helper for fetching terms for a (rel, concept) pair
    def fetch_terms(rel, concept):
        mapped_uri = concept if concept.startswith('/c/') else concept
        url = f'https://api.conceptnet.io/query?rel=/r/{rel}&end={mapped_uri}'
        try:
            resp = session.get(url, timeout=5)
            edges = resp.json().get('edges', [])
            return rel, {edge['start']['label'].lower() for edge in edges if edge.get('start',{}).get('label')}
        except Exception as e:
            logger.error('Relation fetch failed for %s,%s: %s', rel, concept, e)
            return rel, set()

    # Parallel positive fetch
    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = [executor.submit(fetch_terms, rel, concept) for rel, concept in positive_pairs]
        for future in as_completed(futures):
            rel, terms = future.result()
            for term in terms:
                score_counter[term] += 1

    candidates = set(score_counter)

    # Parallel negative fetch
    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = [executor.submit(fetch_terms, rel, concept) for rel, concept in negative_pairs]
        for future in as_completed(futures):
            rel, neg_terms = future.result()
            candidates -= neg_terms

    # Only keep candidates matching a strict majority of positive relations
    if positive_pairs:
        threshold_val = (len(positive_pairs)//threshold)-1
        return [term for term, score in score_counter.items() if score >= threshold_val]
    return []

def augmentedGuessingGameByRelations(objectList, res, promptG, promptO, promptC):
    """
    Alternative 20 Questions game: tracks ConceptNet relations from Q&A,
    samples candidates based on those relations, and asks for maximum
    information-gain relation next.
    """
    objects = open(objectList, 'r', encoding='utf8').read().split("\n")
    # Track (relation, concept) pairs
    global ig_records
    positive_pairs = []
    negative_pairs = []
    # --- Ensure IG CSV header exists before starting rounds ---
    import os
    import csv
    ig_csv = 'ig_records.csv'
    if not os.path.exists(ig_csv):
        with open(ig_csv, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['ig', 'questions_remaining'])
    # --- End header creation ---
    for target in objects:
        guesser = MultiChatBot(promptG, model=MODEL, provider=PROVIDER)
        oracle = ChatBot(promptO + target + ".", model=MODEL, provider=PROVIDER)
        responseHistory = []
        responseO = ''
        while ((("Incorrect" in responseO or "not correct" in responseO or "not Correct" not in responseO)
               and len(responseHistory) < 100)):
            # Generate candidates by relations
            if len(responseHistory) == 0:
                responseG = "Guesser said: What is the object related to?" #Relation with the greatest information gain as measured by conceptnet edges
            else:
                candidates = generateCandidatesByRelations(positive_pairs, negative_pairs)
                best_attr, ig = aggregateAttributes(candidates, use_entropy=True)
                # Compose input: instruct Guesser to pick next relation for max information gain
                enriched = (
                        "Aggregated candidate attribute from external analysis: " + best_attr + ".\n" +
                        "Conversation History:\n" + "\n".join(responseHistory) + "\n" +
                        "Based on this clue, please generate your next open-ended clarifying question or explicit guess. For example, if these attributes include 'metal, sharp', you might ask: 'What material is it made of?' or 'What is the object's function?' or guess: 'Is it a knife?'",
                        "Do not explain your reasoning in your guess, only say your question. You will start each message with Guesser said: . If you guess wrong you will ask more questions about the object until you have enough information to guess again."
                    )
                try:
                    responseG = guesser(enriched)
                except Exception as e:
                    logger.error("Guesser error: %s", e)
                    responseG = ""
                # Record IG and questions remaining before filtering candidates
                questions_asked = len(responseHistory) // 2
                questions_remaining = 100 - questions_asked
                ig_records.append({'ig': ig, 'questions_remaining': questions_remaining})
                # Immediately append this record to CSV
                with open(ig_csv, 'a', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow([ig, questions_remaining])
            responseHistory.append(responseG)
            # Ask Oracle
            try:
                responseO = oracle(responseG)
            except Exception as e:
                logger.error("Oracle error: %s", e)
                responseO = "Oracle said: "
            responseHistory.append(responseO)

            # Semantic concept extraction: map Oracle response to multiple (relation, URI) pairs
            answer_text_full = responseO.split(':', 1)[1].strip()
            answer_text = answer_text_full.lower()
            mappings = map_response_to_uris(answer_text_full)
            # Add all high-similarity mappings
            if answer_text.startswith('no'):
                for relation, uri in mappings:
                    negative_pairs.append((relation, uri))
            # Check for correct guess
            elif "correct" in responseO.lower() or target in answer_text.split(" "):
                break
            else:
                for relation, uri in mappings:
                    positive_pairs.append((relation, uri))
        # Write results
        num_questions = len(responseHistory) // 2
        open(res, 'a', encoding='utf8').write(f"{target},{num_questions},{responseHistory}\n")


augmentedGuessingGameByRelations("objects.txt", "info_gain_results.txt", opg, opo, [0])